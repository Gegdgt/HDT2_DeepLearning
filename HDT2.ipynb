{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de Trabajo 2\n",
    "\n",
    "Gabriel García - 21352        \n",
    "Luis Montenegro - 21699"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1 - Experimentación Practica\n",
    "\n",
    "En esta actividad, implementará y comparará diferentes funciones de pérdida y técnicas de regularización utilizando PyTorch. Utilizará el conjunto de datos de Iris para una tarea de clasificación y una arquitectura básica de red neuronal de feedforward. El objetivo es observar cómo las diferentes opciones impactan la convergencia y el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Preparación del conjunto de datos\n",
    "\n",
    "Cargue el conjunto de datos de Iris utilizando bibliotecas como sklearn.datasets. Luego, divida el conjunto de datos en conjuntos de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjuntos de entrenamiento y validación (80% entrenamiento, 20% validación)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conjuntos de datos y cargadores de datos\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 120 muestras\n",
      "Tamaño del conjunto de validación: 30 muestras\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las formas de los conjuntos de datos para verificación\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(train_loader.dataset)} muestras\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(val_loader.dataset)} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Arquitectura modelo\n",
    "Cree una red neuronal feedforward simple utilizando nn.Module de PyTorch. Luego, defina capa de entrada, capas ocultas y capa de salida. Después, elija las funciones de activación y el número de neuronas por capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la red neuronal\n",
    "class SimpleFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleFeedforwardNN, self).__init__()\n",
    "        # Definir la capa de entrada\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        # Definir la primera capa oculta\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        # Definir la capa de salida\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Aplicar activación ReLU a la capa de entrada\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Aplicar activación ReLU a la primera capa oculta\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Aplicar activación Softmax a la capa de salida para obtener probabilidades\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleFeedforwardNN(\n",
      "  (fc1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del modelo\n",
    "input_size = 4  # El conjunto de datos de Iris tiene 4 características de entrada\n",
    "hidden_size1 = 10  # Número de neuronas en la primera capa oculta\n",
    "hidden_size2 = 8   # Número de neuronas en la segunda capa oculta\n",
    "output_size = 3    # El conjunto de datos de Iris tiene 3 clases de salida\n",
    "\n",
    "# Instanciar el modelo\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Mostrar la arquitectura del modelo\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Funciones de Pérdida\n",
    "Utilice diferentes funciones de pérdida comunes como Cross-Entropy Loss y MSE para clasificación. Entrene el modelo con diferentes funciones de pérdida y registre las pérdidas de entrenamiento y test. Debe utilizar al menos 3 diferentes funciones. Es decir, procure que su código sea capaz de parametrizar el uso de diferentes funciones de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ajuste para CrossEntropyLoss y NLLLoss\n",
    "            if isinstance(criterion, (nn.CrossEntropyLoss, nn.NLLLoss)):\n",
    "                # Las etiquetas deben ser de tipo Long para estas funciones\n",
    "                labels = labels.long()\n",
    "            \n",
    "            # Ajuste para MSELoss: convertir labels a one-hot encoding\n",
    "            elif isinstance(criterion, nn.MSELoss):\n",
    "                labels = F.one_hot(labels, num_classes=output_size).float()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Ajuste para CrossEntropyLoss y NLLLoss\n",
    "                if isinstance(criterion, (nn.CrossEntropyLoss, nn.NLLLoss)):\n",
    "                    labels = labels.long()\n",
    "                \n",
    "                # Ajuste para MSELoss: convertir labels a one-hot encoding\n",
    "                elif isinstance(criterion, nn.MSELoss):\n",
    "                    labels = F.one_hot(labels, num_classes=output_size).float()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el modelo y optimizador\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las funciones de pérdida\n",
    "loss_functions = {\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'MSELoss': nn.MSELoss(),\n",
    "    'NLLLoss': nn.NLLLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando con CrossEntropyLoss...\n",
      "Epoch 1/20, Training Loss: 1.0529, Validation Loss: 1.0444\n",
      "Epoch 2/20, Training Loss: 1.0391, Validation Loss: 1.0309\n",
      "Epoch 3/20, Training Loss: 1.0264, Validation Loss: 1.0171\n",
      "Epoch 4/20, Training Loss: 1.0123, Validation Loss: 1.0037\n",
      "Epoch 5/20, Training Loss: 0.9989, Validation Loss: 0.9899\n",
      "Epoch 6/20, Training Loss: 0.9844, Validation Loss: 0.9754\n",
      "Epoch 7/20, Training Loss: 0.9692, Validation Loss: 0.9596\n",
      "Epoch 8/20, Training Loss: 0.9523, Validation Loss: 0.9424\n",
      "Epoch 9/20, Training Loss: 0.9344, Validation Loss: 0.9240\n",
      "Epoch 10/20, Training Loss: 0.9150, Validation Loss: 0.9042\n",
      "Epoch 11/20, Training Loss: 0.8949, Validation Loss: 0.8829\n",
      "Epoch 12/20, Training Loss: 0.8722, Validation Loss: 0.8606\n",
      "Epoch 13/20, Training Loss: 0.8492, Validation Loss: 0.8368\n",
      "Epoch 14/20, Training Loss: 0.8242, Validation Loss: 0.8118\n",
      "Epoch 15/20, Training Loss: 0.7978, Validation Loss: 0.7853\n",
      "Epoch 16/20, Training Loss: 0.7708, Validation Loss: 0.7583\n",
      "Epoch 17/20, Training Loss: 0.7430, Validation Loss: 0.7316\n",
      "Epoch 18/20, Training Loss: 0.7161, Validation Loss: 0.7048\n",
      "Epoch 19/20, Training Loss: 0.6889, Validation Loss: 0.6782\n",
      "Epoch 20/20, Training Loss: 0.6622, Validation Loss: 0.6521\n",
      "\n",
      "Entrenando con MSELoss...\n",
      "Epoch 1/20, Training Loss: 2.3271, Validation Loss: 2.3149\n",
      "Epoch 2/20, Training Loss: 2.3120, Validation Loss: 2.3016\n",
      "Epoch 3/20, Training Loss: 2.2993, Validation Loss: 2.2900\n",
      "Epoch 4/20, Training Loss: 2.2877, Validation Loss: 2.2802\n",
      "Epoch 5/20, Training Loss: 2.2775, Validation Loss: 2.2717\n",
      "Epoch 6/20, Training Loss: 2.2692, Validation Loss: 2.2633\n",
      "Epoch 7/20, Training Loss: 2.2602, Validation Loss: 2.2556\n",
      "Epoch 8/20, Training Loss: 2.2529, Validation Loss: 2.2483\n",
      "Epoch 9/20, Training Loss: 2.2458, Validation Loss: 2.2420\n",
      "Epoch 10/20, Training Loss: 2.2395, Validation Loss: 2.2363\n",
      "Epoch 11/20, Training Loss: 2.2338, Validation Loss: 2.2316\n",
      "Epoch 12/20, Training Loss: 2.2292, Validation Loss: 2.2277\n",
      "Epoch 13/20, Training Loss: 2.2251, Validation Loss: 2.2246\n",
      "Epoch 14/20, Training Loss: 2.2220, Validation Loss: 2.2225\n",
      "Epoch 15/20, Training Loss: 2.2198, Validation Loss: 2.2210\n",
      "Epoch 16/20, Training Loss: 2.2181, Validation Loss: 2.2197\n",
      "Epoch 17/20, Training Loss: 2.2167, Validation Loss: 2.2188\n",
      "Epoch 18/20, Training Loss: 2.2155, Validation Loss: 2.2178\n",
      "Epoch 19/20, Training Loss: 2.2143, Validation Loss: 2.2167\n",
      "Epoch 20/20, Training Loss: 2.2132, Validation Loss: 2.2158\n",
      "\n",
      "Entrenando con NLLLoss...\n",
      "Epoch 1/20, Training Loss: 1.1419, Validation Loss: 1.1299\n",
      "Epoch 2/20, Training Loss: 1.1229, Validation Loss: 1.1132\n",
      "Epoch 3/20, Training Loss: 1.1040, Validation Loss: 1.0976\n",
      "Epoch 4/20, Training Loss: 1.0866, Validation Loss: 1.0815\n",
      "Epoch 5/20, Training Loss: 1.0680, Validation Loss: 1.0647\n",
      "Epoch 6/20, Training Loss: 1.0492, Validation Loss: 1.0461\n",
      "Epoch 7/20, Training Loss: 1.0282, Validation Loss: 1.0268\n",
      "Epoch 8/20, Training Loss: 1.0065, Validation Loss: 1.0058\n",
      "Epoch 9/20, Training Loss: 0.9826, Validation Loss: 0.9829\n",
      "Epoch 10/20, Training Loss: 0.9576, Validation Loss: 0.9579\n",
      "Epoch 11/20, Training Loss: 0.9306, Validation Loss: 0.9303\n",
      "Epoch 12/20, Training Loss: 0.9015, Validation Loss: 0.9007\n",
      "Epoch 13/20, Training Loss: 0.8699, Validation Loss: 0.8698\n",
      "Epoch 14/20, Training Loss: 0.8379, Validation Loss: 0.8375\n",
      "Epoch 15/20, Training Loss: 0.8050, Validation Loss: 0.8051\n",
      "Epoch 16/20, Training Loss: 0.7719, Validation Loss: 0.7729\n",
      "Epoch 17/20, Training Loss: 0.7390, Validation Loss: 0.7411\n",
      "Epoch 18/20, Training Loss: 0.7070, Validation Loss: 0.7104\n",
      "Epoch 19/20, Training Loss: 0.6754, Validation Loss: 0.6809\n",
      "Epoch 20/20, Training Loss: 0.6456, Validation Loss: 0.6529\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y registrar pérdidas para cada función de pérdida\n",
    "results = {}\n",
    "for loss_name, criterion in loss_functions.items():\n",
    "    print(f'\\nEntrenando con {loss_name}...')\n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)\n",
    "    results[loss_name] = {'train_loss': train_losses, 'val_loss': val_losses}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Técnicas de Regularización\n",
    "Utilice distintas técnicas de regularización como L1, L2 y dropout. Entrene el modelo con y sin técnicas de regularización y observe el impacto en el overfitting y la generalización. Debe utilizar al menos 3 diferentes técnicas. Es decir, procure que su código sea capaz de parametrizar el uso de diferentes técnicas de regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando con No Regularization...\n",
      "Epoch 1/20, Training Loss: 0.0337, Validation Loss: 0.0130\n",
      "Epoch 2/20, Training Loss: 0.0044, Validation Loss: -0.0154\n",
      "Epoch 3/20, Training Loss: -0.0270, Validation Loss: -0.0459\n",
      "Epoch 4/20, Training Loss: -0.0602, Validation Loss: -0.0795\n",
      "Epoch 5/20, Training Loss: -0.0973, Validation Loss: -0.1163\n",
      "Epoch 6/20, Training Loss: -0.1369, Validation Loss: -0.1568\n",
      "Epoch 7/20, Training Loss: -0.1818, Validation Loss: -0.2022\n",
      "Epoch 8/20, Training Loss: -0.2319, Validation Loss: -0.2535\n",
      "Epoch 9/20, Training Loss: -0.2894, Validation Loss: -0.3105\n",
      "Epoch 10/20, Training Loss: -0.3509, Validation Loss: -0.3759\n",
      "Epoch 11/20, Training Loss: -0.4237, Validation Loss: -0.4501\n",
      "Epoch 12/20, Training Loss: -0.5082, Validation Loss: -0.5357\n",
      "Epoch 13/20, Training Loss: -0.6054, Validation Loss: -0.6367\n",
      "Epoch 14/20, Training Loss: -0.7186, Validation Loss: -0.7551\n",
      "Epoch 15/20, Training Loss: -0.8497, Validation Loss: -0.8918\n",
      "Epoch 16/20, Training Loss: -1.0002, Validation Loss: -1.0463\n",
      "Epoch 17/20, Training Loss: -1.1706, Validation Loss: -1.2248\n",
      "Epoch 18/20, Training Loss: -1.3629, Validation Loss: -1.4279\n",
      "Epoch 19/20, Training Loss: -1.5876, Validation Loss: -1.6548\n",
      "Epoch 20/20, Training Loss: -1.8326, Validation Loss: -1.9102\n",
      "\n",
      "Entrenando con L1 Regularization...\n",
      "Epoch 1/20, Training Loss: 0.0426, Validation Loss: 0.0213\n",
      "Epoch 2/20, Training Loss: 0.0097, Validation Loss: -0.0117\n",
      "Epoch 3/20, Training Loss: -0.0239, Validation Loss: -0.0468\n",
      "Epoch 4/20, Training Loss: -0.0604, Validation Loss: -0.0844\n",
      "Epoch 5/20, Training Loss: -0.1001, Validation Loss: -0.1244\n",
      "Epoch 6/20, Training Loss: -0.1424, Validation Loss: -0.1682\n",
      "Epoch 7/20, Training Loss: -0.1878, Validation Loss: -0.2166\n",
      "Epoch 8/20, Training Loss: -0.2389, Validation Loss: -0.2710\n",
      "Epoch 9/20, Training Loss: -0.2980, Validation Loss: -0.3329\n",
      "Epoch 10/20, Training Loss: -0.3639, Validation Loss: -0.4051\n",
      "Epoch 11/20, Training Loss: -0.4403, Validation Loss: -0.4867\n",
      "Epoch 12/20, Training Loss: -0.5270, Validation Loss: -0.5833\n",
      "Epoch 13/20, Training Loss: -0.6286, Validation Loss: -0.6969\n",
      "Epoch 14/20, Training Loss: -0.7531, Validation Loss: -0.8293\n",
      "Epoch 15/20, Training Loss: -0.8938, Validation Loss: -0.9872\n",
      "Epoch 16/20, Training Loss: -1.0620, Validation Loss: -1.1715\n",
      "Epoch 17/20, Training Loss: -1.2559, Validation Loss: -1.3814\n",
      "Epoch 18/20, Training Loss: -1.4828, Validation Loss: -1.6170\n",
      "Epoch 19/20, Training Loss: -1.7306, Validation Loss: -1.8874\n",
      "Epoch 20/20, Training Loss: -2.0159, Validation Loss: -2.1893\n",
      "\n",
      "Entrenando con L2 Regularization...\n",
      "Epoch 1/20, Training Loss: -0.0010, Validation Loss: -0.0243\n",
      "Epoch 2/20, Training Loss: -0.0329, Validation Loss: -0.0584\n",
      "Epoch 3/20, Training Loss: -0.0680, Validation Loss: -0.0954\n",
      "Epoch 4/20, Training Loss: -0.1049, Validation Loss: -0.1358\n",
      "Epoch 5/20, Training Loss: -0.1476, Validation Loss: -0.1783\n",
      "Epoch 6/20, Training Loss: -0.1906, Validation Loss: -0.2262\n",
      "Epoch 7/20, Training Loss: -0.2394, Validation Loss: -0.2798\n",
      "Epoch 8/20, Training Loss: -0.2967, Validation Loss: -0.3397\n",
      "Epoch 9/20, Training Loss: -0.3616, Validation Loss: -0.4085\n",
      "Epoch 10/20, Training Loss: -0.4361, Validation Loss: -0.4883\n",
      "Epoch 11/20, Training Loss: -0.5224, Validation Loss: -0.5846\n",
      "Epoch 12/20, Training Loss: -0.6276, Validation Loss: -0.6978\n",
      "Epoch 13/20, Training Loss: -0.7500, Validation Loss: -0.8306\n",
      "Epoch 14/20, Training Loss: -0.8927, Validation Loss: -0.9843\n",
      "Epoch 15/20, Training Loss: -1.0575, Validation Loss: -1.1631\n",
      "Epoch 16/20, Training Loss: -1.2471, Validation Loss: -1.3660\n",
      "Epoch 17/20, Training Loss: -1.4651, Validation Loss: -1.5904\n",
      "Epoch 18/20, Training Loss: -1.7018, Validation Loss: -1.8433\n",
      "Epoch 19/20, Training Loss: -1.9657, Validation Loss: -2.1279\n",
      "Epoch 20/20, Training Loss: -2.2649, Validation Loss: -2.4438\n",
      "\n",
      "Entrenando con Dropout Regularization...\n",
      "Epoch 1/20, Training Loss: 0.1158, Validation Loss: 0.1212\n",
      "Epoch 2/20, Training Loss: 0.1164, Validation Loss: 0.1072\n",
      "Epoch 3/20, Training Loss: 0.0950, Validation Loss: 0.0936\n",
      "Epoch 4/20, Training Loss: 0.0586, Validation Loss: 0.0794\n",
      "Epoch 5/20, Training Loss: 0.0780, Validation Loss: 0.0653\n",
      "Epoch 6/20, Training Loss: 0.0459, Validation Loss: 0.0508\n",
      "Epoch 7/20, Training Loss: 0.0386, Validation Loss: 0.0358\n",
      "Epoch 8/20, Training Loss: 0.0323, Validation Loss: 0.0200\n",
      "Epoch 9/20, Training Loss: 0.0154, Validation Loss: 0.0024\n",
      "Epoch 10/20, Training Loss: -0.0151, Validation Loss: -0.0180\n",
      "Epoch 11/20, Training Loss: -0.0334, Validation Loss: -0.0399\n",
      "Epoch 12/20, Training Loss: -0.0557, Validation Loss: -0.0630\n",
      "Epoch 13/20, Training Loss: -0.0791, Validation Loss: -0.0885\n",
      "Epoch 14/20, Training Loss: -0.1250, Validation Loss: -0.1190\n",
      "Epoch 15/20, Training Loss: -0.1443, Validation Loss: -0.1518\n",
      "Epoch 16/20, Training Loss: -0.1785, Validation Loss: -0.1888\n",
      "Epoch 17/20, Training Loss: -0.1984, Validation Loss: -0.2320\n",
      "Epoch 18/20, Training Loss: -0.2722, Validation Loss: -0.2822\n",
      "Epoch 19/20, Training Loss: -0.3228, Validation Loss: -0.3438\n",
      "Epoch 20/20, Training Loss: -0.4236, Validation Loss: -0.4139\n"
     ]
    }
   ],
   "source": [
    "# Modificar la arquitectura del modelo para incluir dropout\n",
    "class RegularizedFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.5):\n",
    "        super(RegularizedFeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Aplicar dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Aplicar dropout\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Función para aplicar regularización L1 y L2\n",
    "def apply_regularization(model, l1_lambda=0.0, l2_lambda=0.0):\n",
    "    l1_penalty = torch.tensor(0.0, requires_grad=True)\n",
    "    l2_penalty = torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if l1_lambda > 0:\n",
    "            l1_penalty = l1_penalty + torch.sum(torch.abs(param))\n",
    "        if l2_lambda > 0:\n",
    "            l2_penalty = l2_penalty + torch.sum(param ** 2)\n",
    "    \n",
    "    return l1_lambda * l1_penalty + l2_lambda * l2_penalty\n",
    "\n",
    "# Modificar la función de entrenamiento para incluir regularización\n",
    "def train_model_with_regularization(model, train_loader, val_loader, criterion, optimizer, epochs=20, l1_lambda=0.0, l2_lambda=0.0):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Aplicar regularización L1 y L2\n",
    "            regularization_loss = apply_regularization(model, l1_lambda, l2_lambda)\n",
    "            loss += regularization_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Aplicar regularización L1 y L2 durante la validación también\n",
    "                regularization_loss = apply_regularization(model, l1_lambda, l2_lambda)\n",
    "                loss += regularization_loss\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Entrenar con diferentes técnicas de regularización\n",
    "regularization_params = {\n",
    "    'No Regularization': {'l1_lambda': 0.0, 'l2_lambda': 0.0, 'dropout_rate': 0.0},\n",
    "    'L1 Regularization': {'l1_lambda': 1e-5, 'l2_lambda': 0.0, 'dropout_rate': 0.0},\n",
    "    'L2 Regularization': {'l1_lambda': 0.0, 'l2_lambda': 1e-4, 'dropout_rate': 0.0},\n",
    "    'Dropout Regularization': {'l1_lambda': 0.0, 'l2_lambda': 0.0, 'dropout_rate': 0.5},\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for reg_name, params in regularization_params.items():\n",
    "    print(f'\\nEntrenando con {reg_name}...')\n",
    "    model = RegularizedFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size, dropout_rate=params['dropout_rate'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_losses, val_losses = train_model_with_regularization(model, train_loader, val_loader, criterion, optimizer, epochs=20, l1_lambda=params['l1_lambda'], l2_lambda=params['l2_lambda'])\n",
    "    results[reg_name] = {'train_loss': train_losses, 'val_loss': val_losses}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
