{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de Trabajo 2\n",
    "\n",
    "Gabriel García - 21352        \n",
    "Luis Montenegro - 21699"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1 - Experimentación Practica\n",
    "\n",
    "En esta actividad, implementará y comparará diferentes funciones de pérdida y técnicas de regularización utilizando PyTorch. Utilizará el conjunto de datos de Iris para una tarea de clasificación y una arquitectura básica de red neuronal de feedforward. El objetivo es observar cómo las diferentes opciones impactan la convergencia y el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Preparación del conjunto de datos\n",
    "\n",
    "Cargue el conjunto de datos de Iris utilizando bibliotecas como sklearn.datasets. Luego, divida el conjunto de datos en conjuntos de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjuntos de entrenamiento y validación (80% entrenamiento, 20% validación)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conjuntos de datos y cargadores de datos\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 120 muestras\n",
      "Tamaño del conjunto de validación: 30 muestras\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las formas de los conjuntos de datos para verificación\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(train_loader.dataset)} muestras\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(val_loader.dataset)} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Arquitectura modelo\n",
    "Cree una red neuronal feedforward simple utilizando nn.Module de PyTorch. Luego, defina capa de entrada, capas ocultas y capa de salida. Después, elija las funciones de activación y el número de neuronas por capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura de la red neuronal\n",
    "class SimpleFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleFeedforwardNN, self).__init__()\n",
    "        # Definir la capa de entrada\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        # Definir la primera capa oculta\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        # Definir la capa de salida\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Aplicar activación ReLU a la capa de entrada\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Aplicar activación ReLU a la primera capa oculta\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Aplicar activación Softmax a la capa de salida para obtener probabilidades\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleFeedforwardNN(\n",
      "  (fc1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del modelo\n",
    "input_size = 4  # El conjunto de datos de Iris tiene 4 características de entrada\n",
    "hidden_size1 = 10  # Número de neuronas en la primera capa oculta\n",
    "hidden_size2 = 8   # Número de neuronas en la segunda capa oculta\n",
    "output_size = 3    # El conjunto de datos de Iris tiene 3 clases de salida\n",
    "\n",
    "# Instanciar el modelo\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Mostrar la arquitectura del modelo\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Funciones de Pérdida\n",
    "Utilice diferentes funciones de pérdida comunes como Cross-Entropy Loss y MSE para clasificación. Entrene el modelo con diferentes funciones de pérdida y registre las pérdidas de entrenamiento y test. Debe utilizar al menos 3 diferentes funciones. Es decir, procure que su código sea capaz de parametrizar el uso de diferentes funciones de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ajuste para CrossEntropyLoss y NLLLoss\n",
    "            if isinstance(criterion, (nn.CrossEntropyLoss, nn.NLLLoss)):\n",
    "                # Las etiquetas deben ser de tipo Long para estas funciones\n",
    "                labels = labels.long()\n",
    "            \n",
    "            # Ajuste para MSELoss: convertir labels a one-hot encoding\n",
    "            elif isinstance(criterion, nn.MSELoss):\n",
    "                labels = F.one_hot(labels, num_classes=output_size).float()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Ajuste para CrossEntropyLoss y NLLLoss\n",
    "                if isinstance(criterion, (nn.CrossEntropyLoss, nn.NLLLoss)):\n",
    "                    labels = labels.long()\n",
    "                \n",
    "                # Ajuste para MSELoss: convertir labels a one-hot encoding\n",
    "                elif isinstance(criterion, nn.MSELoss):\n",
    "                    labels = F.one_hot(labels, num_classes=output_size).float()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el modelo y optimizador\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las funciones de pérdida\n",
    "loss_functions = {\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'MSELoss': nn.MSELoss(),\n",
    "    'NLLLoss': nn.NLLLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando con CrossEntropyLoss...\n",
      "Epoch 1/20, Training Loss: 1.0529, Validation Loss: 1.0444\n",
      "Epoch 2/20, Training Loss: 1.0391, Validation Loss: 1.0309\n",
      "Epoch 3/20, Training Loss: 1.0264, Validation Loss: 1.0171\n",
      "Epoch 4/20, Training Loss: 1.0123, Validation Loss: 1.0037\n",
      "Epoch 5/20, Training Loss: 0.9989, Validation Loss: 0.9899\n",
      "Epoch 6/20, Training Loss: 0.9844, Validation Loss: 0.9754\n",
      "Epoch 7/20, Training Loss: 0.9692, Validation Loss: 0.9596\n",
      "Epoch 8/20, Training Loss: 0.9523, Validation Loss: 0.9424\n",
      "Epoch 9/20, Training Loss: 0.9344, Validation Loss: 0.9240\n",
      "Epoch 10/20, Training Loss: 0.9150, Validation Loss: 0.9042\n",
      "Epoch 11/20, Training Loss: 0.8949, Validation Loss: 0.8829\n",
      "Epoch 12/20, Training Loss: 0.8722, Validation Loss: 0.8606\n",
      "Epoch 13/20, Training Loss: 0.8492, Validation Loss: 0.8368\n",
      "Epoch 14/20, Training Loss: 0.8242, Validation Loss: 0.8118\n",
      "Epoch 15/20, Training Loss: 0.7978, Validation Loss: 0.7853\n",
      "Epoch 16/20, Training Loss: 0.7708, Validation Loss: 0.7583\n",
      "Epoch 17/20, Training Loss: 0.7430, Validation Loss: 0.7316\n",
      "Epoch 18/20, Training Loss: 0.7161, Validation Loss: 0.7048\n",
      "Epoch 19/20, Training Loss: 0.6889, Validation Loss: 0.6782\n",
      "Epoch 20/20, Training Loss: 0.6622, Validation Loss: 0.6521\n",
      "\n",
      "Entrenando con MSELoss...\n",
      "Epoch 1/20, Training Loss: 2.3271, Validation Loss: 2.3149\n",
      "Epoch 2/20, Training Loss: 2.3120, Validation Loss: 2.3016\n",
      "Epoch 3/20, Training Loss: 2.2993, Validation Loss: 2.2900\n",
      "Epoch 4/20, Training Loss: 2.2877, Validation Loss: 2.2802\n",
      "Epoch 5/20, Training Loss: 2.2775, Validation Loss: 2.2717\n",
      "Epoch 6/20, Training Loss: 2.2692, Validation Loss: 2.2633\n",
      "Epoch 7/20, Training Loss: 2.2602, Validation Loss: 2.2556\n",
      "Epoch 8/20, Training Loss: 2.2529, Validation Loss: 2.2483\n",
      "Epoch 9/20, Training Loss: 2.2458, Validation Loss: 2.2420\n",
      "Epoch 10/20, Training Loss: 2.2395, Validation Loss: 2.2363\n",
      "Epoch 11/20, Training Loss: 2.2338, Validation Loss: 2.2316\n",
      "Epoch 12/20, Training Loss: 2.2292, Validation Loss: 2.2277\n",
      "Epoch 13/20, Training Loss: 2.2251, Validation Loss: 2.2246\n",
      "Epoch 14/20, Training Loss: 2.2220, Validation Loss: 2.2225\n",
      "Epoch 15/20, Training Loss: 2.2198, Validation Loss: 2.2210\n",
      "Epoch 16/20, Training Loss: 2.2181, Validation Loss: 2.2197\n",
      "Epoch 17/20, Training Loss: 2.2167, Validation Loss: 2.2188\n",
      "Epoch 18/20, Training Loss: 2.2155, Validation Loss: 2.2178\n",
      "Epoch 19/20, Training Loss: 2.2143, Validation Loss: 2.2167\n",
      "Epoch 20/20, Training Loss: 2.2132, Validation Loss: 2.2158\n",
      "\n",
      "Entrenando con NLLLoss...\n",
      "Epoch 1/20, Training Loss: 1.1419, Validation Loss: 1.1299\n",
      "Epoch 2/20, Training Loss: 1.1229, Validation Loss: 1.1132\n",
      "Epoch 3/20, Training Loss: 1.1040, Validation Loss: 1.0976\n",
      "Epoch 4/20, Training Loss: 1.0866, Validation Loss: 1.0815\n",
      "Epoch 5/20, Training Loss: 1.0680, Validation Loss: 1.0647\n",
      "Epoch 6/20, Training Loss: 1.0492, Validation Loss: 1.0461\n",
      "Epoch 7/20, Training Loss: 1.0282, Validation Loss: 1.0268\n",
      "Epoch 8/20, Training Loss: 1.0065, Validation Loss: 1.0058\n",
      "Epoch 9/20, Training Loss: 0.9826, Validation Loss: 0.9829\n",
      "Epoch 10/20, Training Loss: 0.9576, Validation Loss: 0.9579\n",
      "Epoch 11/20, Training Loss: 0.9306, Validation Loss: 0.9303\n",
      "Epoch 12/20, Training Loss: 0.9015, Validation Loss: 0.9007\n",
      "Epoch 13/20, Training Loss: 0.8699, Validation Loss: 0.8698\n",
      "Epoch 14/20, Training Loss: 0.8379, Validation Loss: 0.8375\n",
      "Epoch 15/20, Training Loss: 0.8050, Validation Loss: 0.8051\n",
      "Epoch 16/20, Training Loss: 0.7719, Validation Loss: 0.7729\n",
      "Epoch 17/20, Training Loss: 0.7390, Validation Loss: 0.7411\n",
      "Epoch 18/20, Training Loss: 0.7070, Validation Loss: 0.7104\n",
      "Epoch 19/20, Training Loss: 0.6754, Validation Loss: 0.6809\n",
      "Epoch 20/20, Training Loss: 0.6456, Validation Loss: 0.6529\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y registrar pérdidas para cada función de pérdida\n",
    "results = {}\n",
    "for loss_name, criterion in loss_functions.items():\n",
    "    print(f'\\nEntrenando con {loss_name}...')\n",
    "    model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)\n",
    "    results[loss_name] = {'train_loss': train_losses, 'val_loss': val_losses}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Técnicas de Regularización\n",
    "Utilice distintas técnicas de regularización como L1, L2 y dropout. Entrene el modelo con y sin técnicas de regularización y observe el impacto en el overfitting y la generalización. Debe utilizar al menos 3 diferentes técnicas. Es decir, procure que su código sea capaz de parametrizar el uso de diferentes técnicas de regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
